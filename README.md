# Web Scraping

This repository documents my personal progress and experiments while learning **web scraping** ‚Äî from basics to more advanced techniques.

### What you'll find here

- Simple scripts using **requests + BeautifulSoup**
- Handling JavaScript-heavy sites with **Playwright**, **Selenium**, or **Scrapy**
- Dealing with anti-bot measures (headers, proxies, delays, CAPTCHA avoidance basics)
- Parsing different formats (HTML, JSON APIs, XML)
- Data cleaning & saving (to CSV, JSON, SQLite, etc.)
- Small real-world scraping projects (e.g. product prices, news headlines, job listings)
- Notes, gotchas, reference links, and lessons learned

Most code is written in **Python**, but I might add examples in other languages later.

### Status

üöß **Work in progress** ‚Äî this repo is mainly a personal learning log.  
Code quality varies (from quick&dirty experiments to more structured attempts).  
Expect incomplete scripts, refactoring in progress, and lots of comments explaining *why* something is done a certain way (or why it failed).

### Primary Hosting & Mirroring

- **Primary location**: Self-hosted Gitea instance
- **Mirror**: [https://github.com/mhmdinan/web-scraping](https://github.com/mhmdinan/web-scraping)

This repo lives primarily on my personal self-hosted Gitea instance.  
The GitHub copy is **automatically mirrored** (push mirror) for better discoverability and as a backup.

‚Üí Issues, pull requests, stars, etc. will be wasted as GitHub is read-only / mirror-only.

### Tools & Technologies I'm exploring

- Python: requests, httpx, beautifulsoup4, lxml, parsel
- Browser automation: playwright, selenium, puppeteer (Node.js)
- Scraping frameworks: scrapy, scrapy-playwright
- Anti-detection helpers: curl-impersonate, undetected-chromedriver, residential proxies
- Data handling: pandas, sqlite3, json

### License

MIT License ‚Äî feel free to use any code snippet for your own learning or projects.

But please be responsible: respect robots.txt, don't overload servers, don't scrape personal data without permission, etc.

Happy scraping (and learning)! üï∏Ô∏è

